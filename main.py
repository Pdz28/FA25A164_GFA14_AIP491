from __future__ import annotations

import os
from typing import Dict

from fastapi import FastAPI, File, UploadFile, Request, Query
from fastapi.responses import JSONResponse, Response
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from PIL import Image
from contextlib import asynccontextmanager

from app.services.inference import InferenceService
from weights.load_weight import load_weight


BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # project root
APP_DIR = os.path.join(BASE_DIR, "app")
STATIC_DIR = os.path.join(APP_DIR, "static")
TEMPLATES_DIR = os.path.join(APP_DIR, "templates")
UPLOAD_DIR = os.path.join(STATIC_DIR, "uploads")
OUTPUT_DIR = os.path.join(STATIC_DIR, "outputs")
WEIGHTS_DIR = os.path.join(BASE_DIR, "weights")

os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Try to fetch weight from Hugging Face into weights/ before service starts
    try:
        load_weight(weights_dir=WEIGHTS_DIR)
    except Exception as e:
        # Non-fatal: continue without downloaded weights
        print(f"[startup] Skipped HF download: {e}")

    # Startup: create inference service and attach to app.state
    app.state.service = InferenceService(weights_dir=WEIGHTS_DIR)
    try:
        yield
    finally:
        # Shutdown: cleanup hooks if available
        svc = getattr(app.state, "service", None)
        if svc and getattr(svc, "gradcam", None):
            try:
                svc.gradcam.remove_hooks()
            except Exception:
                pass


app = FastAPI(title="CNN+Swin Fusion - Grad-CAM Demo", lifespan=lifespan)
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")
templates = Jinja2Templates(directory=TEMPLATES_DIR)


# No deprecated on_event; service is created in lifespan


@app.get("/")
async def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})


@app.get("/health")
async def health(request: Request):
    """Lightweight health/status endpoint used by the UI to show which model pieces loaded.

    Returns 200 when the service object is available, 503 if not yet ready.
    """
    service: InferenceService | None = getattr(request.app.state, "service", None)
    if service is None:
        return JSONResponse({"ready": False, "message": "service not ready"}, status_code=503)

    effnet_loaded = getattr(service, "effnet", None) is not None
    return {
        "ready": True,
        "device": str(service.device),
        "loaded_weights": getattr(service, "loaded_weights_info", ""),
        "effnet_loaded": bool(effnet_loaded),
    }


@app.get("/flutter_service_worker.js")
async def flutter_sw():
    # Some browsers may request this due to a previous PWA installation.
    # Serve a minimal no-op service worker to avoid noisy 404s.
    js = (
        "// noop service worker generated by FastAPI\n"
        "self.addEventListener('install', event => { self.skipWaiting && self.skipWaiting(); });\n"
        "self.addEventListener('activate', event => { });\n"
        "self.addEventListener('fetch', event => { /* pass-through */ });\n"
    )
    return Response(content=js, media_type="application/javascript")


@app.get("/.well-known/appspecific/com.chrome.devtools.json")
async def chrome_devtools_hint():
    """Some Chrome/Edge DevTools versions probe this path. Return an empty JSON to avoid noisy 404s."""
    return JSONResponse({}, status_code=200)

@app.get("/favicon.ico")
async def favicon_quiet():
    """Avoid 404 noise when the browser requests /favicon.ico."""
    return Response(status_code=204)


@app.post("/predict")
async def predict(
    request: Request,
    file: UploadFile = File(...),
    mode: str = Query("fusion"),
    token_stage: str = Query("7", description="Swin token stage: 7/14/28/56; default is 7 (last)") ,
    enhance: bool = Query(False, description="Enhanced contrast (per-pixel alpha + percentile clip)"),
    per_pixel: bool = Query(False, description="Use per-pixel alpha blending for overlay"),
    alpha_min: float = Query(0.0, description="Minimum per-pixel alpha when per_pixel is true"),
    alpha_max: float = Query(0.6, description="Maximum per-pixel alpha when per_pixel is true"),
) -> Dict:
    service: InferenceService | None = getattr(request.app.state, "service", None)
    if service is None:
        return JSONResponse({"error": "Service not ready"}, status_code=503)

    # Save upload
    filename = file.filename or "upload.png"
    name, _ = os.path.splitext(filename)
    safe_name = name.replace(" ", "_")
    upload_path = os.path.join(UPLOAD_DIR, f"{safe_name}.png")

    img = Image.open(file.file).convert("RGB")
    img.save(upload_path)

    try:
        result = service.predict_with_gradcam(
            img,
            OUTPUT_DIR,
            mode=mode,
            token_stage=token_stage,
            enhance=enhance,
            per_pixel=per_pixel,
            alpha_min=alpha_min,
            alpha_max=alpha_max,
        )
    except RuntimeError as e:
        # Return 400 for known client errors (e.g., EffNet not available)
        msg = str(e)
        if "EffNet" in msg or "EffNet visualization" in msg:
            return JSONResponse({"error": msg}, status_code=400)
        # Fallback: return error to client
        return JSONResponse({"error": msg}, status_code=400)
    except Exception as e:
        # Unexpected server error during prediction: log traceback and return JSON error
        import traceback

        tb = traceback.format_exc()
        print(f"[predict] Unexpected error:\n{tb}")
        return JSONResponse({"error": "Internal server error during prediction", "detail": str(e)}, status_code=500)

    # Convert to URLs for the frontend
    def to_url(p: str) -> str:
        return "/static/" + os.path.relpath(p, STATIC_DIR).replace("\\", "/")

    return {
        "pred_label": result["pred_label"],
        "probs": result["probs"],
        "uploaded_url": to_url(upload_path),
        "gradcam_url": to_url(result["gradcam_path"]),
    }


if __name__ == "__main__":
    # Convenience for local development: allow `python main.py` to run the server.
    # In production or when using auto-reload, prefer: `uvicorn main:app --reload`.
    import uvicorn

    host = os.getenv("HOST", "127.0.0.1")
    port = int(os.getenv("PORT", "8000"))
    reload = os.getenv("RELOAD", "1") in {"1", "true", "True"}
    uvicorn.run("main:app", host=host, port=port, reload=reload)

# Training Module - SimplifiedHá»‡ thá»‘ng training Ä‘Æ¡n giáº£n, táº¥t cáº£ trong 1 file duy nháº¥t.## ğŸ“ Cáº¥u trÃºc```training/â”œâ”€â”€ train.py          # Script training chÃ­nh (all-in-one: 300 dÃ²ng)â””â”€â”€ README.md         # File nÃ y```## ğŸš€ Sá»­ dá»¥ng### Cháº¡y training```bashpython training/train.py```### Chá»‰nh configMá»Ÿ `training/train.py` vÃ  sá»­a class `Config`:```pythonclass Config:    # Data paths    train_dir = "/path/to/train"    val_dir = "/path/to/val"        # Hyperparameters    batch_size = 32    epochs = 101    classifier_lr = 2e-4    # ...```## âœ¨ TÃ­nh nÄƒng- âœ… **DualBranchDataset**: Dataset cho CNN + ViT (tÃ­ch há»£p trong train.py)- âœ… **LoRA**: Efficient fine-tuning cho Swin- âœ… **Progressive Unfreezing**: Má»Ÿ khÃ³a EfficientNet á»Ÿ epoch 8- âœ… **Early Stopping**: Dá»«ng khi khÃ´ng cáº£i thiá»‡n (40 epochs)- âœ… **Checkpoint**: LÆ°u best model theo val_loss- âœ… **Data Augmentation**: Flip, Rotation, ColorJitter## ğŸ“Š Output```Epoch 10/101  Train: Loss=0.3245 | Acc=85.67%  Val:   Loss=0.3891 | Acc=82.34%  ğŸ‰ Best model saved!```Checkpoint lÆ°u táº¡i: `checkpoints/best_model.pth`## ğŸ”§ TÃ¹y chá»‰nh### Thay Ä‘á»•i data path```pythonconfig.train_dir = "/your/train/path"config.val_dir = "/your/val/path"```### Thay Ä‘á»•i hyperparameters```pythonconfig.batch_size = 64config.epochs = 150config.classifier_lr = 3e-4```### Thay Ä‘á»•i LoRA config```pythonconfig.lora_r = 32  # TÄƒng rankconfig.lora_alpha = 64```## ğŸ“¦ So sÃ¡nh vá»›i phiÃªn báº£n phá»©c táº¡p| Aspect | Complex (17 files) | Simple (1 file) ||--------|-------------------|-----------------|| **Files** | 17 files, 7 folders | 1 file || **Lines** | ~3000 LOC | ~300 LOC || **Learning curve** | Steep | Flat || **Customization** | Callbacks, configs | Direct edit || **Maintainability** | High (modular) | Medium (monolithic) || **Best for** | Production | Learning/Quick experiments |---**Simple & Clean! ğŸ¯**